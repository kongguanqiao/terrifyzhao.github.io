---
layout: post
title: '循环神经网络RNN进阶'
subtitle: 'RNN BP算法讲解'
date: 2018-02-13
categories: 神经网络
cover: ''
tags: 神经网络 RNN
---


## 1、前言

在上一篇[循环神经网络RNN入门](https://terrifyzhao.github.io/2018/02/07/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E5%85%A5%E9%97%A8.html)中，我们简单讲解了RNN的运用背景与结构，这一节，我们会继续深入讲解RNN，包括RNN的BP与不同类型的RNN


## 2、RNN的反向传播BPTT 

RNN反向传播算法的思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数。由于我们是基于时间反向传播，所以RNN的反向传播有一个很炫酷的名字BPTT(back-propagation through time)。当然这里的BPTT和DNN也有很大的不同点，即这里所有的参数在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。

首先我们先回顾下前向传播，隐藏层为：

$${h^{(t)}} = \sigma(Ux^{(t)}+Wh^{(t-1)}+b)$$

其中$\sigma$为RNN的激活函数，一般为$tanh$, $b$为偏移量。
输出中层为：

$$o^{(t)} = Vh^{(t)}+c $$

激活后：

$$\hat{y^{(t)}} = \sigma(o^{(t)}) $$

注意，通常由于RNN是识别类的分类模型，所以上面这个激活函数一般是softmax。

接下来我们来看反向传播。为了简化描述，这里的损失函数我们为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。对于RNN，由于我们在序列的每个位置都有损失函数，我们假设每个序列的损失函数为${\ L^{(t)}}$ 因此最终的损失${\ L}$为：

$${\ L} = \sum_{t=1}^{\tau}L^{(t)}$$

其中$V, c$的梯度计算是比较简单的：

$$\frac{\partial{L}}{\partial{c}}=\sum_{t=1}^{\tau}\frac{\partial{L^{(t)}}}{\partial{o^{(t)}}}\frac{\partial{o^{(t)}}}{\partial{c^{(t)}}}=\sum_{t=1}^{\tau}\hat{y^{(t)}}-y^{(t)}$$

$$\frac{\partial{L}}{\partial{V}}=\sum_{t=1}^{\tau}\frac{\partial{L^{(t)}}}{\partial{o^{(t)}}}\frac{\partial{o^{(t)}}}{\partial{V^{(t)}}}=\sum_{t=1}^{\tau}(\hat{y^{(t)}}-y^{(t)})(h^{(t)})^T$$

对于$W，U, b$的梯度，计算起来相对复杂些，在某一序列位置t的梯度损失由当前位置的输出对应的梯度损失和序列索引位置t+1t+1时的梯度损失两部分共同决定，对于WW在某一序列位置t的梯度损失需要反向传播一步步的计算。我们定义序列索引tt位置的隐藏状态的梯度为：

$$\delta^{t}=\frac{\partial{L}}{\partial{h^{t}}}=\frac{\partial{L^{(t)}}}{\partial{o^{(t)}}}\frac{\partial{o^{(t)}}}{\partial{h^{(t)}}} + \frac{\partial{L^{(t)}}}{\partial{h^{(t+1)}}}\frac{\partial{h^{(t+1)}}}{\partial{h^{(t)}}}=V^{T}(\hat{y^{(\tau)}}-y^{(\tau)})$$

有了$\delta^{(t)}$,计算$W,U,b$就容易了，这里给出$W,U,b$的梯度计算表达式：

$$\frac{\partial{L}}{\partial{W}} = \sum_{t=1}^{\tau}\frac{\partial{L}}{\partial{h^{(t)}}}\frac{\partial{h^{(t)}}}{\partial{W}}=\sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{(t)}(h^{(t-1)})^T$$

$$\frac{\partial{L}}{\partial{b}} = \sum_{t=1}^{\tau}\frac{\partial{L}}{\partial{h^{(t)}}}\frac{\partial{h^{(t)}}}{\partial{b}}=\sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{(t)}$$

$$\frac{\partial{L}}{\partial{U}} = \sum_{t=1}^{\tau}\frac{\partial{L}}{\partial{h^{(t)}}}\frac{\partial{h^{(t)}}}{\partial{U}}=\sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)\delta^{(t)}(x^{(t)})^T$$

## 3、不同类型的RNN
<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E8%BF%9B%E9%98%B6/rnn1.jpg"/>

RNN一共有五种不同的类型

+ 一对一 这种类型不需要使用RNN
+ 一对多 例如音乐生成
+ 多对一 例如情感分析，读取一段影评，输出用户是否喜欢这个电影
+ 多对多 输入条目数=输出条目数 例如机器翻译
+ 多对多 输入条目数!=输出条目数 例如输入一段视频，输出视频的一些标签

只要合理的使用RNN的基本模块，把他们组合起来，就可以构建出各种各样的模型。




