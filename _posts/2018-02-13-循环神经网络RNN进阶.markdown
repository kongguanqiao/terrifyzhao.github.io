---
layout: post
title: '循环神经网络RNN进阶'
subtitle: 'RNN进阶讲解'
date: 2018-02-13
categories: 神经网络
cover: ''
tags: 机器学习 神经网络
---


## 1、前言

在上一篇[循环神经网络RNN入门](https://terrifyzhao.github.io/2018/02/07/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E5%85%A5%E9%97%A8.html)中，我们简单讲解了RNN的运用背景与结构，这一节，我们会继续深入讲解RNN，包括RNN的BP、五中结构的RNN以及GRU,LSTM


## 2、RNN的反向传播BPTT 

RNN反向传播算法的思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数。由于我们是基于时间反向传播，所以RNN的反向传播有一个很炫酷的名字BPTT(back-propagation through time)。当然这里的BPTT和DNN也有很大的不同点，即这里所有的参数在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。

这里的损失函数我们为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。对于RNN，由于我们在序列的每个位置都有损失函数，每个序列的损失函数为${\ L^{(t)}}$ 因此最终的损失${\ L}$为：

${\ L}$ = $\sum_1^{\tau}L^{(t)}$

![](bptt1.jpg)
<img src="bptt1.jpg"/>

