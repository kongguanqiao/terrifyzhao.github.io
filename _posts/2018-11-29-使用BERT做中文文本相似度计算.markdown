---
layout: post
title: '最简单的BERT使用指南-使用BERT做中文文本相似度计算'
subtitle: '使用BERT做中文文本相似度计算'
date: 2018-11-29
categories: NLP 文本相似度
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-09-17-Rasa%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%9701/cover.png'
tags: NLP
---

## 简介

最近Google推出了NLP大杀器BERT，BERT（Transformer双向编码器表示）是Google AI语言研究人员最近发表的一篇论文。它通过在各种NLP任务中呈现最先进的结果，包括问答系统、自然语言推理等，引起了机器学习社区的轰动。


本文不会去讲解BERT的原理，如果您还不清楚什么是BERT建议先参阅Google的[论文]('https://arxiv.org/abs/1810.04805')或者其他博文，本文主要目的在于教会大家怎么使用BERT的预训练模型。

在开始使用之前，我们先简单介绍一下到底什么是BERT，大家也可以去BERT的[github]('https://github.com/google-research/bert')上进行详细的了解。在CV问题中，目前已经有了很多成熟的预训练模型供大家使用，我们只需要修改结尾的FC层或根据实际场景添加softmax层，也就是我们常说的迁移学习。那在NLP领域是否有类似的方法呢，答案是肯定的，BERT就是这样的预训练模型。对于NLP的正常流程来说，我们需要做一些预处理，例如分词、W2V等，BERT包含所有的预训练过程，只需要提供文本数据即可，接下来我们会基于NLP常用的文本相似度计算问题来介绍如何使用BERT。

## 下载预训练模型

谷歌提供了以下几个版本的BERT模型，每个模型的参数都做了简单的说明，中文的预训练模型在11月3日的时候提供了，这里我们只需要用到中文的版本，[点击下载]('https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip')

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic1.jpg">

下载下来的文件包括以下内容

* TensorFlow 用来保存预训练模型的三个 checkpoint 文件(bert_model.ckpt.xxx) 
* 字典文件，用于做ID的映射 (vocab.txt) 
* 配置文件，该文件的参数是fine-tuning时模型用到的，可自行调整 (bert_config.json) 

## 编写相似度计算的代码

模型准备好后就可以编写代码了，我们先把BERT的[github]('https://github.com/google-research/bert')代码clone下来，之后我们的代码编写会基于`run_classifier.py`文件，我们看下代码的结构

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic2.jpg", width=300, align='center'>

可以看到有好几个`xxxProcessor`的类，这些类都有同一个父类`DataProcessor`，其中`DataProcessor`提供了4个抽象方法，如图

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic3.jpg", width=500, align='center'>

顾名思义，`Processor`就是用来获取对应的训练集、验证集、测试集的数据与label的数据，并把这些数据喂给BERT的，而我们要做的就是自定义新的Processor并重写这4个方法，也就是说我们只需要提供我们自己场景对应的数据。这里我自定义了一个名叫SimProcessor的类，我们简单看一下

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic4.jpg", width=600, align='center'>

读取的数据需要封装成一个`InputExample`的对象并添加到list中，注意这里有一个guid的参数，这个参数是必填的，是用来区分每一条数据的。

`get_labels`方法返回的是一个数组，因为相似度维度可以理解为分类问题，所以返回的标签只有0和1，注意，这里我返回的是参数是字符串，所以在重写获取数据的方法时`InputExample`中的label也要传字符串的数据，可以看到上图中我对`label`做了一个`str()`的处理。

这里我偷了个懒，没有编写验证集的代码，建议大家添加上去，运行的时候是否做验证集的计算是有参数控制的，我们在下文会讲解到。

接下来我们还需要给`Processor`加一个名字，让我们的在运行时告诉代码我们要执行哪一个`Processor`


ok，到这里我们已经把`Processor`编写好了，接下来就是运行代码了，我们来看下`mian`函数。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic5.jpg", width=300, align='center'>

可以看到，在执行`run_classifier.py`时需要先输入这5个必填参数，这里我们对参数做一个简单的说明

| 参数 | 说明 |
| ------ | ------ |
|data_dir | 训练数据的地址 |
|task_name | processor名字 |
|data_dir | 训练数据的地址 |
|data_dir | 训练数据的地址 |
|data_dir | 训练数据的地址 |

