---
layout: post
title: 'BERT中文文本相似度计算与文本分类'
subtitle: '最简单的BERT使用指南'
date: 2018-11-29
categories: NLP 文本相似度
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/cover.jpeg'
tags: NLP
---

## 简介

最近Google推出了NLP大杀器BERT，BERT（Transformer双向编码器表示）是Google AI语言研究人员最近发表的一篇论文。它通过在各种NLP任务中呈现最先进的结果，包括问答系统、自然语言推理等，引起了机器学习社区的轰动。


本文不会去讲解BERT的原理，如果您还不清楚什么是BERT建议先参阅Google的[论文]('https://arxiv.org/abs/1810.04805')或者其他博文，本文主要目的在于教会大家怎么使用BERT的预训练模型。

在开始使用之前，我们先简单介绍一下到底什么是BERT，大家也可以去BERT的[github]('https://github.com/google-research/bert')上进行详细的了解。在CV问题中，目前已经有了很多成熟的预训练模型供大家使用，我们只需要修改结尾的FC层或根据实际场景添加softmax层，也就是我们常说的迁移学习。那在NLP领域是否有类似的方法呢，答案是肯定的，BERT就是这样的预训练模型。对于NLP的正常流程来说，我们需要做一些预处理，例如分词、W2V等，BERT包含所有的预训练过程，只需要提供文本数据即可，接下来我们会基于NLP常用的文本相似度计算问题来介绍如何使用BERT。

## 下载预训练模型

谷歌提供了以下几个版本的BERT模型，每个模型的参数都做了简单的说明，中文的预训练模型在11月3日的时候提供了，这里我们只需要用到中文的版本，[点击下载]('https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip')

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic1.jpg"/>

下载下来的文件包括以下内容

* TensorFlow 用来保存预训练模型的三个 checkpoint 文件(bert_model.ckpt.xxx) 
* 字典文件，用于做ID的映射 (vocab.txt) 
* 配置文件，该文件的参数是fine-tuning时模型用到的，可自行调整 (bert_config.json) 

## 编写代码

模型准备好后就可以编写代码了，我们先把BERT的[github]('https://github.com/google-research/bert')代码clone下来，之后我们的代码编写会基于`run_classifier.py`文件，我们看下代码的结构

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic2.jpg" width="300" >

可以看到有好几个`xxxProcessor`的类，这些类都有同一个父类`DataProcessor`，其中`DataProcessor`提供了4个抽象方法，如图

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic3.jpg" width="500"/>

顾名思义，`Processor`就是用来获取对应的训练集、验证集、测试集的数据与label的数据，并把这些数据喂给BERT的，而我们要做的就是自定义新的Processor并重写这4个方法，也就是说我们只需要提供我们自己场景对应的数据。这里我自定义了一个名叫SimProcessor的类，我们简单看一下

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic4.jpg"  width="600"/>

读取的数据需要封装成一个`InputExample`的对象并添加到list中，注意这里有一个guid的参数，这个参数是必填的，是用来区分每一条数据的。是否进行训练集、验证集、测试集的计算，在执行代码时会有参数控制，我们下文会讲，所以这里的抽象方法也并不是需要全部都重写，但是为了体验一个完整的流程， 建议大家还是简单写一下。

`get_labels`方法返回的是一个数组，因为相似度问题可以理解为分类问题，所以返回的标签只有0和1，注意，这里我返回的是参数是字符串，所以在重写获取数据的方法时`InputExample`中的label也要传字符串的数据，可以看到上图中我对`label`做了一个`str()`的处理。

接下来还需要给`Processor`加一个名字，让我们的在运行时告诉代码我们要执行哪一个`Processor`，如图我自定义的叫做sim

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic6.jpg" width="300">

## 训练模型

ok，到这里我们已经把`Processor`编写好了，接下来就是运行代码了，我们来看下`run_classifier.py`的执行过程。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-11-29-%E4%BD%BF%E7%94%A8BERT%E5%81%9A%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/pic5.jpg" width="300">

可以看到，在执行`run_classifier.py`时需要先输入这5个必填参数，这里我们对参数做一个简单的说明

| 参数 | 说明 |
| ------ | ------ |
|data_dir | 训练数据的地址 |
|task_name | processor的名字 |
|vocab_file | 字典地址，用默认提供的就可以了，当然也可以自定义 |
|bert_config_file | 配置文件 |
|output_dir | 模型的输出地址 |

当然还有一些其他的参数，这里给出官方提供的运行参数

```
export BERT_BASE_DIR=/Users/joe/Desktop/chinese_L-12_H-768_A-12

export MY_DATASET=/Users/joe/Desktop/bert_data 

python run_classifier.py \
  --data_dir=$MY_DATASET \
  --task_name=sim \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --output_dir=/tmp/sim_model/ \
  --do_train=true \
  --do_eval=true \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=5e-5\
  --num_train_epochs=2.0 
```

这里再补充下以下三个可选参数说明

| 参数 | 说明 |
| ------ | ------ |
|do_train | 是否做fine-tuning，默认为false，如果为true必须重写获取训练集的方法 |
|do_eval | 是否运行验证集，默认为false，如果为true必须重写获取验证集的方法 |
|dopredict | 是否做预测，默认为false，如果为true必须重写获取测试集的方法 |

## 预测

执行以上的代码即可训练我们自己的模型了，如果需要使用模型来进行预测，可执行以下命令

```
python run_classifier.py \
  --task_name=sim \
  --do_predict=true \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=/tmp/sim_model \
  --max_seq_length=128 \
  --output_dir=/tmp/output/
```

当然，我们需要在`data_dir`下有测试数据，测试完成后会在`output_dir`路径下生成一个`test_results.tsv`文件，该文件包含了测试用例和相似度probabilities


## 总结
除了相似度计算，以上的代码完全能够用来做文本二分类，你也可以根据自己的需求来修改Processor，更多的细节大家可以参阅github与源码。

