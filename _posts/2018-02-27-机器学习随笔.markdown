---
layout: post
title: '机器学习随笔'
subtitle: '面试考点'
date: 2018-02-27
categories: 机器学习 面试
cover: ''
tags: 机器学习
---


1、正则化 regularization
控制参数的幅度，限制参数搜索空间
$L_1$正则 $|\theta|$

$L_2$正则 $\theta^2$
一般采用$L_2$正则，方便求导


2、逻辑回归不使用平方损失函数是因为不是凸函数，其次，线性回归的代价函数采用的是最小二乘法，最小二乘法实际上需要数据服从正态分布，而逻辑回归属于二项分布，即0-1分布，因此没法采用最小二乘法，而是采用极大似然估计

3、逻辑回归的主要目的是去找判定边界Decision Boundary 逻辑回归是对概率进行拟合

4、过拟合

+ 找更多数据来学习
+ 增大正则化系数
+ dropout
+ regularization
+ batch normalizatin

5、欠拟合

+ 找更多的特征
+ 减小正则化系数

6、模型效果优化方法

+ Bagging，每一个模型只读取一部分数据，例如随机森林，最终结果求平均或者投票

+ Stacking，最终结果会作为特征再次进行训练

+ Blending，弱化版本的Stacking，特征训练较简单，可能是投票或平均

+ Adaboost,每次都把前一次训练错的内容加权训练，最后将所有的训练结果合并在一起

+ Gradient Boosting Tree，串行模型，会过拟合

单个模型太强，那就使用并行，防止过拟合，单个模型太弱，那就使用串行

7、流程

preprocessing - learning - evaluation - predict

8、使用交叉验证来选择最佳的超参数，可以使用`grid_search.GridSearchCV`

9、如果一个函数单调递减并且有下界，那么一定是收敛函数，例如k-means的代价函数

10、准确率与召回率

假设推荐新闻，一共推荐给用户10条，用户感兴趣的新闻是12条，用户只点击了推荐的10条中的4条，那么

$$准确率 = \frac{4}{10}$$

$$召回率 = \frac{4}{12}$$

11、机器学历学派

+ 连接主义 例如神经网络
+ 符号主义 基于逻辑的学习
+ 统计学习 基于统计学原理，现有理论，再有算法，例如SVM
+ 概率图模型 贝叶斯网络

12、最大似然估计是从概率角度来想这个问题，直观理解，似然函数在给定参数的条件下就是观测到一组数据realization的概率（或者概率密度）。最大似然函数的思想就是什么样的参数才能使我们观测到目前这组数据的概率是最大的。

先验概率:是指根据以往经验和分析得到的概率.[1]

后验概率:事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小[1]

后验概率 = 先验概率 * 最大似然估计概率

13、xgboost处理缺失值的方法和其他树模型不同。根据作者Tianqi Chen在论文[1]中章节3.4的介绍，xgboost把缺失值当做稀疏矩阵来对待，本身的在节点分裂时不考虑的缺失值的数值。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。具体的介绍可以参考[2,3]。

14、总结来看，对于有缺失值的数据在经过缺失值处理后：

+ 数据量很小，用朴素贝叶斯
+ 数据量适中或者较大，用树模型，优先 xgboost
+ 数据量较大，也可以用神经网络
+ 避免使用距离度量相关的模型，如KNN和SVM

15、概率

$$ P(X|Y) = \frac{P(X)P(Y|X)}{P(Y)}$$

$$P(X|Y) 后验概率、P(Y) 先验概率
$$

+ 先验概率：直觉确定的值，例如抛硬币的概率为0.5
+ 后验概率：经过观察后的值，例如抛硬币100次，40次朝上，则向上的后验概率为0.4

狼人杀就是一个贝叶斯游戏，通过不断的观察，计算出后验概率


+ 生成模型：结果与P(X)有关系

朴素贝叶斯、隐马尔科夫模型、GAN

+ 判别模型：

其他机器学习模型，即不需要根据其他概率来得出最终结果

期望可以理解为平均数

16、ROC曲线
例如我们要预测是否是垃圾邮件，首先对所有的预测计算结果进行排序，选择预测率最大的值，与真实结果进行比较计算出TPR与FPR，再选择前两个值进行TPR与FPR的计算，最后根据TPR与FPR进行ROC曲线绘制，AUC的值为ROC的面积。

AUC最差结果是0.5，如果是0.1其实是较好的效果，只需要把0、1互换，就可以得到0.9的结果

其次，还可以绘制prediction-recall曲线，召回率越高，准确率越高的结果也就越好

17、最小二乘法与梯度下降的区别

最小二乘法提供的仅是一种描述loss function（损失函数）的方案，它本身不是一个解最优解的工具。形象点来说，最小二乘法告诉我们目标是什么。而至于如何来实现这个目标，分两种情况。

+ 线性模型 
直接求导，令导数为0

+ 非线性模型
采用迭代法，例如梯度下降、牛顿法。梯度下降是一种典型的数值方法，它的唯一目的，就是用迭代的方法求出局部最优解。


18、为啥牛顿法迭代次数少

牛顿法是二阶收敛的，而梯度下降法是一阶收敛。二阶其实相当于考虑了梯度的梯度，所以相对更快。 

比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。

19、牛顿法的缺点

+ 牛顿法起始点不能离局部极小点太远，否则很可能不会收敛。
+ 牛顿法需要计算Hessian矩阵，矩阵的总元素个数为$\frac{k(k+1)}{2}$，如果在交叉项较多的情况下，计算效率低下。


20、怎么判断函数是凸函数

+ 一阶泰勒展开 <= 原函数
+ 二阶导数 > 0


21、matplotlib图使用场景

曲线图：走势
灰度图/直方图：查看数据分布，针对单个维度
散点图：2个维度之间的关联关系，关联度大会呈线性关系
箱式/箱线图：统计边界是否受异常值影响、容忍度的大小




