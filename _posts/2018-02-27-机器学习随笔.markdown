---
layout: post
title: '机器学习随笔'
subtitle: '随笔'
date: 2018-02-27
categories: 机器学习
cover: ''
tags: 机器学习
---


1、正则化 regularization
控制参数的幅度，限制参数搜索空间
$L_1$正则 $|\theta|$

$L_2$正则 $\theta^2$
一般采用$L_2$正则，方便求导


2、逻辑回归不使用平方损失函数是因为不是凸函数

3、逻辑回归的主要目的是去找判定边界Decision Boundary 逻辑回归是对概率进行拟合

4、过拟合

+ 找更多数据来学习
+ 增大正则化系数

5、欠拟合

+ 找更多的特征
+ 减小正则化系数

6、模型效果优化方法

+ Bagging，每一个模型只读取一部分数据，例如随机森林，最终结果求平均或者投票

+ Stacking，最终结果会作为特征再次进行训练

+ Blending，弱化版本的Stacking，特征训练较简单，可能是投票或平均

+ Adaboost,每次都把前一次训练错的内容加权训练，最后将所有的训练结果合并在一起

+ Gradient Boosting Tree，串行模型，会过拟合

单个模型太强，那就使用并行，防止过拟合，单个模型太弱，那就使用串行

7、流程

preprocessing - learning - evaluation - predict

8、使用交叉验证来选择最佳的超参数，可以使用`grid_search.GridSearchCV`








