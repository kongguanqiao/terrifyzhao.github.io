---
layout: post
title: '机器学习随笔'
subtitle: '随笔'
date: 2018-02-27
categories: 机器学习
cover: ''
tags: 机器学习
---


1、正则化 regularization
控制参数的幅度，限制参数搜索空间
$L_1$正则 $|\theta|$

$L_2$正则 $\theta^2$
一般采用$L_2$正则，方便求导


2、逻辑回归不使用平方损失函数是因为不是凸函数

3、逻辑回归的主要目的是去找判定边界Decision Boundary 逻辑回归是对概率进行拟合

4、过拟合

+ 找更多数据来学习
+ 增大正则化系数
+ dropout
+ regularization
+ batch normalizatin

5、欠拟合

+ 找更多的特征
+ 减小正则化系数

6、模型效果优化方法

+ Bagging，每一个模型只读取一部分数据，例如随机森林，最终结果求平均或者投票

+ Stacking，最终结果会作为特征再次进行训练

+ Blending，弱化版本的Stacking，特征训练较简单，可能是投票或平均

+ Adaboost,每次都把前一次训练错的内容加权训练，最后将所有的训练结果合并在一起

+ Gradient Boosting Tree，串行模型，会过拟合

单个模型太强，那就使用并行，防止过拟合，单个模型太弱，那就使用串行

7、流程

preprocessing - learning - evaluation - predict

8、使用交叉验证来选择最佳的超参数，可以使用`grid_search.GridSearchCV`

9、如果一个函数单调递减并且有下界，那么一定是收敛函数，例如k-means的代价函数

10、准确率与召回率

假设推荐新闻，一共推荐给用户10条，用户感兴趣的新闻是12条，用户只点击了推荐的10条中的4条，那么

$$准确率 = \frac{4}{10}$$

$$召回率 = \frac{4}{12}$$

11、机器学历学派

+ 连接主义 例如神经网络
+ 符号主义 基于逻辑的学习
+ 统计学习 基于统计学原理，现有理论，再有算法，例如SVM
+ 概率图模型 贝叶斯网络

12、最大似然估计是从概率角度来想这个问题，直观理解，似然函数在给定参数的条件下就是观测到一组数据realization的概率（或者概率密度）。最大似然函数的思想就是什么样的参数才能使我们观测到目前这组数据的概率是最大的。

先验概率:是指根据以往经验和分析得到的概率.[1]

后验概率:事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小[1]

后验概率 = 先验概率 * 最大似然估计概率

13、xgboost处理缺失值的方法和其他树模型不同。根据作者Tianqi Chen在论文[1]中章节3.4的介绍，xgboost把缺失值当做稀疏矩阵来对待，本身的在节点分裂时不考虑的缺失值的数值。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。具体的介绍可以参考[2,3]。

14、总结来看，对于有缺失值的数据在经过缺失值处理后：

+ 数据量很小，用朴素贝叶斯
+ 数据量适中或者较大，用树模型，优先 xgboost
+ 数据量较大，也可以用神经网络
+ 避免使用距离度量相关的模型，如KNN和SVM








