---
layout: post
title: '机器学习随笔'
subtitle: '随笔'
date: 2018-02-27
categories: 机器学习
cover: ''
tags: 机器学习
---


1、正则化 regularization
控制参数的幅度，限制参数搜索空间
$L_1$正则 $|\theta|$

$L_2$正则 $\theta^2$
一般采用$L_2$正则，方便求导


2、逻辑回归不使用平方损失函数是因为不是凸函数

3、逻辑回归的主要目的是去找判定边界Decision Boundary 逻辑回归是对概率进行拟合

4、过拟合

+ 找更多数据来学习
+ 增大正则化系数
+ dropout
+ regularization
+ batch normalizatin

5、欠拟合

+ 找更多的特征
+ 减小正则化系数

6、模型效果优化方法

+ Bagging，每一个模型只读取一部分数据，例如随机森林，最终结果求平均或者投票

+ Stacking，最终结果会作为特征再次进行训练

+ Blending，弱化版本的Stacking，特征训练较简单，可能是投票或平均

+ Adaboost,每次都把前一次训练错的内容加权训练，最后将所有的训练结果合并在一起

+ Gradient Boosting Tree，串行模型，会过拟合

单个模型太强，那就使用并行，防止过拟合，单个模型太弱，那就使用串行

7、流程

preprocessing - learning - evaluation - predict

8、使用交叉验证来选择最佳的超参数，可以使用`grid_search.GridSearchCV`

9、如果一个函数单调递减并且有下界，那么一定是收敛函数，例如k-means的代价函数

10、准确率与召回率

假设推荐新闻，一共推荐给用户10条，用户感兴趣的新闻是12条，用户只点击了推荐的10条中的4条，那么

$$准确率 = \frac{4}{10}$$

$$召回率 = \frac{4}{12}$$

11、机器学历学派

+ 连接主义 例如神经网络
+ 符号主义 基于逻辑的学习
+ 统计学习 基于统计学原理，现有理论，再有算法，例如SVM
+ 概率图模型 贝叶斯网络








