---
layout: post
title: '决策树'
subtitle: '决策树 Ensemble'
date: 2018-02-28
categories: 机器学习
cover: ''
tags: 机器学习
---

## 前言

决策树（Decision Tree）是一种基本的分类与回归方法，该算法结构简明，计算成本低，方便做数据清理，相比神经网络的黑盒模型，其更有说服力，因此工业使用率是很高的。但是决策树也有其缺点，只能做线性分割数据，并且是一个贪婪算法。

## 决策树

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 分类的时候，从根节点开始，对实例的某一个特征进行测试，根据测试结果，将实例分配到其子结点；此时，每一个子结点对应着该特征的一个取值。如此递归向下移动，直至达到叶结点，最后将实例分配到叶结点的类中。 

为了方便理解，我们来看例子

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-28-%E5%86%B3%E7%AD%96%E6%A0%91/dt1.png" width="500" height="500"/>

一个女孩子去相亲，其择偶的标准是年龄，长相，收入，是否是公务员。其中绿色结点表示判断条件，橙色结点表示叶节点即决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示女孩的决策过程。 当把一个人的条件输入到如图的树行结构中，最后就会输出一个是否见面的结果。这就是一个最基础的决策树模型。

决策树的关键是分治思想，能否把数据分成两组，不同的数据点能否被完美区分开(pure)

接下来我们再看一个带有数据的例子，这个是小明14天内是否去打球的数据，我们的任务是根据这14天的数据来判断第15天他是否会去打球。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-28-%E5%86%B3%E7%AD%96%E6%A0%91/dt2.png" width="700" height="500"/>

那么，小明是否打球是否是天气决定的呢

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-28-%E5%86%B3%E7%AD%96%E6%A0%91/dt3.png" width="700" height="330"/>

在天气的条件下，我们把它分为了3类，但是晴天与雨天并没有完美区分开是否打球，也就是说这个是不pure的，因此，我们需要对该类进行进一步的区分。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-28-%E5%86%B3%E7%AD%96%E6%A0%91/dt4.png" width="700" height="330"/>

我们针对晴天用湿度这个特征进行进一步的分类，对雨天用风的强弱来进行进一步分类，最终完美区分开。到这一步，就是我们最终的决策树。

那么，在分类的是否，我们改选用什么特征作为第一个分类呢，是天气还是湿度？这就需要引入一个新的概念，熵Entropy

## 熵Entropy

熵是用来形容物体的一个混乱状态的，如果越混乱熵就越高。例如上面的是否打球，如果是与否各站一半，则熵较大，反之较小，从函数图中可以看到，当取0.5的时候熵最大，取0或1时熵最大。因此熵能计算出按照某个特征分类后纯洁度(pure)有多高。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-28-%E5%86%B3%E7%AD%96%E6%A0%91/dt5.jpg" width="240" height="200"/>

熵Entropy公式如下，其中k指类别数

$$H(S) = -\sum_{i=0}^kP(x)logP(x) $$

例如有6个数据，3个是，3个否

$$H(S) = -\frac{3}{6}log\frac{3}{6}-\frac{3}{6}log\frac{3}{6}=1$$

例如6个数据，6个是，0个否

$$H(S) = -\frac{6}{6}log\frac{6}{6}-\frac{0}{6}log\frac{0}{6}=0$$

那么，选哪一个特征才能选到最大的熵值呢

## 信息增益Information Gain

信息增益是指未分类前的熵减去分类之后的熵的差值，这个值越大越好

$$Gain(S,A) = H(S)-\sum_{v=0}^k H(S_v)$$

例如，我们这里以风为第一个分类，最终结果是9个是，5个否，那么以风为特征的熵为
 
$$H(S) =  -\frac{9}{14}log\frac{9}{14}-\frac{5}{14}log\frac{5}{14}$$

我们是以风的强弱做为分类，强风6个是2个否，弱风3个是2个否，熵分别为
 
$$H(强风) = -\frac{6}{8}log\frac{6}{8}-\frac{2}{8}log\frac{2}{8}$$

$$H(弱风) = -\frac{3}{5}log\frac{3}{5}-\frac{2}{5}log\frac{2}{5}$$

根据公式，最终的信息增益

$$Gain(S,A) = H(S) - H(强风)- H(弱风)$$


## ID3
应用以上两个概念，我们就有现在ID3算法
1.对当前样本集合，计算所有属性的信息增益
2.选择信息增益最大的属性作为测试属性，把测试属性取值相同的样本划为同一个子样本集
3.若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处,否则对子样本集递归调用本算法

但是ID3也有其缺点，它可以把N个数据分成100%纯洁的N组，例如上面的例子，如果按照日期分成14个组，那么也是完成纯洁的，但是却导致了过拟合，且分组毫无意义。

那怎么避免过拟合呢，这里有两个方案

+ 没必要的分类不要做

    例如空气质量是否会影响小明去打球，如果影响率小于25%，那么这样的分类是没有意义的
    
+ 减枝Prune 
     
    例如小明打球的14天数据，我们只用其中10条数据去测试，剩余4条数据作为验证集，当我们用10条数据生成一个树之后，将验证集的数据喂进去，并随机减去一个分类，如果最终仍可以将数据分类成功，那么这个枝就是多余的，是不要分这个类的。
    
## 信息增益比Gain Ratio
对于信息增益来说，也有其缺点，如果按照日期分成14个组，由于分类后的熵都为0，所以最终的信息增益

$$Gain(S,A) = H(S)$$

因此，我们这里又引入了一个新的概念，信息增益比
    
$$GainRatio(S,A) = \frac{Gain(S,A)}{\sum_{v=0}^k H(S_v)}$$

由于结果是一个比值，分母不能为0，从而解决了上面的14个分类的问题。C4.5算法就是在ID3算法的基础上引入了Gain Ratio。

## 决策树的优化
针对于决策树的缺点，在之后的研究中，又提出了一种森林的结构，即多棵树，这里为大家介绍三种多棵决策树的结果

#### Bagging


