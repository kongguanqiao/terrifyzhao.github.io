---
layout: post
title: 'GPT2.0 解读 Language Models are Unsupervised Multitask Learners'
subtitle: 'Language Models are Unsupervised Multitask Learners'
date: 2019-02-18
categories: NLP
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2019-02-18-BERT%E7%BB%93%E6%9E%84/cover.png'
tags: NLP
---



## **简介**

在自然语言处理任务中，如QA、机器翻译、阅读理解、文本摘要、都是在特定数据集上的典型的监督学习任务。我们的模型在一个数百万级别的WebText的数据集上进行非监督训练后，来验证这些NLP任务。当以文档和问题为条件时，在没有使用127000+的训练数据的CoQA数据集上，该模型生成的答案F1 score达到了55，并且4个baseline system中有3个打破了原来的最好成绩。模型的参数个数对于最终的模型效果有很大的影响，我们最大的模型GPT-2的transformer参数量达到了15亿个，并且在WebText这个数据集还存在欠拟合的情况下，已经在8项测试语言模型的数据集上取得了7项最好的成绩。从这个例子中也可以看到，数据量够大和参数较多的情况下模型的效果是有一定的提升的，这也为今后的语言模型指明了一条可行的道路。


## **1.介绍**
目前的监督学习下的机器学习模型在大数据量与高参数量的情况在表现都很不错，但是至今这些模型在数据分布和任务规范程度有轻微改动时都很脆弱、敏感，当前的这些模型其实更适合被描述为一个狭隘的专家，而不是能干的通才，而我们更想去构建一个更加通用的系统能胜任更多的任务，甚至不需要去手动的打标签做微调。

目前主流的机器学习模型都是在指定的任务上去用一部分数据来训练模型，再用一部分不相同但同分布的数据来测试其性能。这样的模式在特定场景效果的确不错，但是对于字幕或者说阅读理解、图像分类这样的任务来说，输入的多样性和不确定性就会把缺点给暴露出来。

我们认为，目前普遍采用的用单一领域的数据来训练单一模型是该问题的罪魁祸首，如果要构建一个泛化能力更强的模型，需要在更广泛的任务和领域上进行训练。目前许多测试基准都已经提出了该理念，例如GLUE、decaNLP。

多任务学习对于提升模型性能来说是很有用的，但是多任务学习在NLP领域还是新生儿。从元学习的角度来看，每个数据对都是从数据集和目标的分布中抽样的单个训练示例。目前的机器学习系统需要成百上千的数据去拟合出更好的函数，这也表明多任务学习需要更多的数据才能达到好的效果，但是一目前的技术来说，继续扩大数据集的规模是很困难的，这也促使我们去探索更多方法来提升多任务学习的效果。

目前效果最好的形式就是预训练模型并采用监督学习做微调的模式，该模式已经有了很长的历史，但是依旧是未来的趋势，起初都是采用词向量来作为特定任务的输入，到后来采用循环神经网络的上下文信息，最近的研究也表明，特定任务的体系结构其实并不是必须的，多个self-attention模块就已经能够满足目前的需要。

对于一些特定的任务可能需要采用监督学习，当只有少量或者没有打过标签的数据的时，另一项研究表明语言模型可通过执行其他一些特定的任务来进行训练，例如常识推理，情感分析。

在本篇论文中，我们会把多任务学习和非监督学习联系起来，并介绍一种更加具有趋向性的方法。我们展示的语言模型能在没有训练样本（没有任何参数和结构的修改）的情况下执行一些下游任务，并且在零样本的情况下泛化性能更强，在一些任务上我们也取得了业界最佳的效果。

## **2.方法**

我们方法的核心是语言建模，语言建模通常是由一组数据构成的无监督分布估计，每一条数据都是可变长度的符号序列组成，由于语言具有自然的顺序排列，因此通常将符号上的联合概率分解为条件概率的乘积

$$ p(x) = \prod^n_{i=1}p(s_n|s_1,...,s_{n-1}) $$


这种方法方便估算$p(x)$以及任何条件的$p(s_{n−k},…,s_n|s_1,...,s_{n-k-1})$。近年来，可以计算这些条件概率的模型的表达能力有了显著的改进，例如采用self-attention结构的transformer。

学习某项单一的任务可以用概率学中的条件概率$p(output|input)$来表示。对于一般的系统应该能执行许多不同的任务，即使是对于同样的输入，也不仅仅只对输入有要求，对待执行的任务也要有一定的要求。因此模型应该是$p(output|input,task)$。这已经在多任务和元学习中得到了不同的体现。任务的调整通常在架构的级别上体现，例如一些独特的任务中的encoders和decoders，或者说在算法级别上，例如MAML的内循环和外循环优化框架。但是在McCann的例子中，在指定的任务中给文本提供了一种灵活的方式，输入输出都是符号序列，例如一个翻译的训练数据能够被改为序列，同样的，阅读理解的训练数据也能改写为序列，McCann演示的MQAN，为训练单一模型能根据输入特定格式的训练数据来推断并执行许多不同的任务提供了可能性。

原则上来说，语言模型也能学习McCann的任务，而不需要对输出哪一个符号做明确的监督学习。虽然监督学习和非监督学习的目标是相同的，但是监督学习只能在子集上进行评估，非监督学习的全局最优解也是监督学习的全局最优解，因此是否我们能够在训练的过程中让非监督学习的目标能够收敛成了最大的问题。初期的实验表明足够大的语言模型是能够执行多任务的，但是其训练速度明显比监督学习的方法慢很多。

其实对话是一种很好的训练方式，但是我们担心他的限制有点过于严格，毕竟我们的数据是WebText，互联网包含的信息太多，这些信息都是被动获取的，并且不存在互动交流。我们认为一个具有足够能力的语言模型不管它的训练方式是什么样的，只要它有足够的能力去学习推断、执行自然语言序列的一些任务就是好模型。如果一个语言模型能把这些做好，实际上他就是在做无监督学习，我们也会在没有增量数据的情况下测试模型多任务上的表现和性能。

### **2.1训练数据集**

之前大部分训练语言模型的工作都是采用单一领域的文本，例如新闻，维基百科，小说。我们的方法鼓励构建尽可能大且多样化的数据集，以便在尽可能多的领域能胜任。

我们写了一个能确保文本质量的网络爬虫，我们最终只用人工筛选过的网页内容，但是人工过滤爬虫内容是很贵的，因此我们只是把这个作为一个起点，我们爬取了Reddit上所有的外部链接，每个链接的karma值至少要有3分。这可以作为一个内容评判指标，用于判断其他用户是否觉得链接内容有趣、有教育意义或仅仅是有趣。

这份数据集我们称之为WebText，其中包含了4500万条链接，我们从HTML的相应内容从提取除了文本内容。在这篇论文中，目前所有用的结果都是用的WebText的初版数据，超过8百万个文档总共40G。我们移除了维基百科的文档，因为WebText中会有一些重复的内容，并且如果训练数据与测试数据重叠过多，可能使分析变得复杂。

### **2.1输入表示**

一个语言模型，应该能计算每一个字符的概率，当前的一些大型规模的语言模型都会包含预处理，例如转小写，分词，处理oov问题，而将Unicode字符串处理为UTF-8字节序列可以很好地解决这一问题。目前来说在大规模的数据集上，字符级别的语言模型相比于词级别的语言模型并没有更大的竞争力，这是我们在WebText数据集上训练字符级别语言模型时发现的性能差距。

BPE是一种介于字符级和字级之间的实用语言模型，它能有效地在频繁符号序列的字级输入和不频繁符号序列的字符级输入之间进行插值，尽管名为BPE，但实际是在处理Unicode编码，而不是字节序列，该方法需要包含所有unicode编码，以便能对所有Unicode字符串建模，在添加任何多符号标记之前，该方法的基本词汇表超过13万。与BPE经常使用的3.2万到6.4万个词汇相比，这个数字大得令人望而却步。相比之下，字节级别的BPE需要的词典大小只有256，


然而，直接将BPE应用于字节序列会导致合并无法达到最优解，因为BPE使用贪婪算法来构建词汇表。我们发现BPE包含了许多像dog这样的常用的词，因为它们出现在许多变体中，比如dog，dog？dog。诸如此类的。该结果将会导致词典词槽分配与模型能力受到限制。为了避免这个问题，我们会防止BPE跨字符类别合并任何字节序列，我们为空格添加了一个异常，它显著地提高了压缩效率，同时只在多个vocab标记之间添加了最小的单词碎片。

这种输入表示允许我们将字级语言模型的经验优势与字节级方法的通用性结合起来。因为我们的方法能给任何一个unicode字符串分配一个概率，这能让我们去评估我们的语言模型在任何数据集上并且不需要去管预处理，分词，或者说词典大小。

### **2.3模型**

我们的语言模型内部采用的是transformer结构，该模型在很大程度上遵循OpenAI GPT模型的细节只是做了一些修改。首先每一个sub-block的输入的layer normalization被移除了，类似于一个预激活的残差网络，并在最后的self-attention模块中添加了layer normalization。采用修正的初始化方法，考虑了模型深度对当前层的影响，我们在初始化权重时将剩余层的权值乘以 $1/\sqrt{N}$ ，其中N是残缺层的数量，词典被扩展到了50257，context的维度从512提高到了1024并且batchsize采用了512。

## **3.实验**

我们一共训练测试了4个模型，其结构如下图：
[](https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2019-02-19-GPT2.0%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/pic1.jpg)

最小的模型规模和GPT是一样的，第二小的和最大的BERT规模一样，我们最大的模型叫做GPT-2，其具有比GPT多一个数量级的参数。每个模型的学习率在5%的WebText保留样本上手工调整以获得最佳的值。所有的模型目前在WebText上都还存在欠拟合的情况，如果给更多的时间去训练的话效果还能进一步的提升。

### **3.1语言建模**

模型验证的第一步当然是零样本任务，我们对WebText训练的语言模型在在一些基本任务零样本的情况下的效果很感兴趣，因为我们的模型是在字节级别操作的，因此并不需要做预处理或者说是分词，我能能够在任何语言模型的基准上进行评估。语言建模数据集上的结果通常以每个标准单元(字符、字节或者词)的平均负样本对数概率的比例或指数作为评判标准。我们的评判标准是：对数概率除以标准节点的个数。对于这些数据集来说，WebText语言模型能明显的测出out-of分布，它能找出没有联系的标点符号、缩进、打乱的句子，甚至<UNK>，不做这个在WebText中出现的次数极少，400亿字节中只出现了26次。下图是我们的模型在各项任务的表现，我们尽可能的把人为的预处理过程移除了。


[](https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2019-02-19-GPT2.0%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/pic1.jpg)



















