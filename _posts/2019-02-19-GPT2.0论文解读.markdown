---
layout: post
title: 'GPT2.0 解读 Language Models are Unsupervised Multitask Learners'
subtitle: 'Language Models are Unsupervised Multitask Learners'
date: 2019-02-18
categories: NLP
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2019-02-18-BERT%E7%BB%93%E6%9E%84/cover.png'
tags: NLP
---



## **简介**

在自然语言处理任务中，如QA、机器翻译、阅读理解、文本摘要、都是在特定数据集上的典型的监督学习任务。我们的模型在一个数百万级别的WebText的数据集上进行非监督训练后，来验证这些NLP任务。当以文档和问题为条件时，在没有使用127000+的训练数据的CoQA数据集上，该模型生成的答案F1 score达到了55，并且4个baseline system中有3个打破了原来的最好成绩。模型的参数个数对于最终的模型效果有很大的影响，我们最大的模型GPT-2的transformer参数量达到了15亿个，并且在WebText这个数据集还存在欠拟合的情况下，已经在8项测试语言模型的数据集上取得了7项最好的成绩。从这个例子中也可以看到，数据量够大和参数较多的情况下模型的效果是有一定的提升的，这也为今后的语言模型指明了一条可行的道路。


## **1.介绍**
目前的监督学习下的机器学习模型在大数据量与高参数量的情况在表现都很不错，但是至今这些模型在数据分布和任务规范程度有轻微改动时都很脆弱、敏感，当前的这些模型其实更适合被描述为一个狭隘的专家，而不是能干的通才，而我们更想去构建一个更加通用的系统能胜任更多的任务，甚至不需要去手动的打标签做微调。

目前主流的机器学习模型都是在指定的任务上去用一部分数据来训练模型，再用一部分不相同但同分布的数据来测试其性能。这样的模式在特定场景效果的确不错，但是对于字幕或者说阅读理解、图像分类这样的任务来说，输入的多样性和不确定性就会把缺点给暴露出来。

我们认为，目前普遍采用的用单一领域的数据来训练单一模型是该问题的罪魁祸首，如果要构建一个泛化能力更强的模型，需要在更广泛的任务和领域上进行训练。目前许多测试基准都已经提出了该理念，例如GLUE、decaNLP。

多任务学习对于提升模型性能来说是很有用的，但是多任务学习在NLP领域还是新生儿。从元学习的角度来看，每个数据对都是从数据集和目标的分布中抽样的单个训练示例。目前的机器学习系统需要成百上千的数据去拟合出更好的函数，这也表明多任务学习需要更多的数据才能达到好的效果，但是一目前的技术来说，继续扩大数据集的规模是很困难的，这也促使我们去探索更多方法来提升多任务学习的效果。

目前效果最好的形式就是预训练模型并采用监督学习做微调的模式，该模式已经有了很长的历史，但是依旧是未来的趋势，起初都是采用词向量来作为特定任务的输入，到后来采用循环神经网络的上下文信息，最近的研究也表明，特定任务的体系结构其实并不是必须的，多个self-attention模块就已经能够满足目前的需要。

对于一些特定的任务可能需要采用监督学习，当只有少量或者没有打过标签的数据的时，另一项研究表明语言模型可通过执行其他一些特定的任务来进行训练，例如常识推理，情感分析。

在本篇论文中，我们会把多任务学习和非监督学习联系起来，并介绍一种更加具有趋向性的方法。我们展示的语言模型能在没有训练样本（没有任何参数和结构的修改）的情况下执行一些下游任务，并且在零样本的情况下泛化性能更强，在一些任务上我们也取得了业界最佳的效果。

## **2.方法**

我们方法的核心是语言建模，语言建模通常是由一组数据构成的无监督分布估计，每一条数据都是可变长度的符号序列组成，由于语言具有自然的顺序排列，因此通常将符号上的联合概率分解为条件概率的乘积

$$
p(x) = \prod^n_{i=1}p(s_n|s_1,...,s_{n-1})
$$


这种方法方便估算$p(x)$以及任何条件的$p(s_{n−k},…,s_n|s_1,...,s_{n-k-1})$。近年来，可以计算这些条件概率的模型的表达能力有了显著的改进，例如采用self-attention结构的transformer。





