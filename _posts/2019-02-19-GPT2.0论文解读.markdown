---
layout: post
title: 'GPT2.0 解读 Language Models are Unsupervised Multitask Learners'
subtitle: 'Language Models are Unsupervised Multitask Learners'
date: 2019-02-18
categories: NLP
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2019-02-18-BERT%E7%BB%93%E6%9E%84/cover.png'
tags: NLP
---



## **简介**

在自然语言处理任务中，如QA、机器翻译、阅读理解、文本摘要、都是在特定数据集上的典型的监督学习任务。我们演示了模型在一个数百万级别的名为WebText的数据集上进行非监督训练后，来验证这些NLP任务。当以文档和问题为条件时，在没有使用127000+的训练数据的CoQA数据集上，该模型生成的答案F1 score达到了55，并且4个baseline system中有3个打破了原来的最好成绩。模型的参数个数对于最终的模型效果有很大的影响，我们最大的模型GPT-2的transformer参数量达到了15亿个，并且在WebText这个数据集还存在欠拟合的情况下，已经在8项测试语言模型的数据集上取得了7项最好的成绩。从这个例子中也可以看到，数据量够大和参数较多的情况下模型的效果是有一定的提升的，这也为今后的语言模型指明了一条可行的道路。


## **介绍**
目前的监督学习下的机器学习模型在大数据量与高参数量的情况在表现都很不错，但是至今这些模型在数据分布和任务规范程度有轻微改动时都很脆弱、敏感，当前的系统其实更适合别描述为一个狭隘的专家，而不是能干的通才，而我们更想去构建一个更加通用的系统能胜任更多的任务，甚至不需要去手动的打标签做增量训练。

目前主流的机器学习模型都是在制定的任务上去用一部分数据来训练模型，再用一部分不相同但同分布的数据来测试其性能。这样的模式在特定场景效果的确不错，但是对于字幕或者说阅读理解、图像分类这样的任务来说，输入的多样性和不确定性就会把缺点给暴露出来。

我们怀疑，目前普遍存在的在单一领域训练单一模型是该问题的罪魁祸首，如果要构建一个泛化能力更强的系统，需要在更广泛的任务和领域上进行性能测试。目前许多测试基准都已经提出了该理念，例如GLUE、decaNLP。

多任务学习是一种很有用的框架，通常情况对性能都会有一定提升，但是多任务学习在NLP领域还是新生儿。从元学习的角度来看，每个数据对都是从数据集和目标的分布中抽样的单个训练示例。目前的机器学习系统需要成百上千的数据去拟合出更好的函数，这也表明多任务学习需要更多的数据才能达到好的效果，但是目前来说继续扩大数据集的规模是很困难的，这也促使我们去探索更多起来方法来实现做任务学习。

目前效果最好的形式就是预训练模型加微调的模式，该模式已经有了很长的历史，但是依旧是未来的趋势，期初都是采用词向量来作为特定学习任务的输入，到后来采用循环神经网络的来提取上下文信息，不过最近的研究也表明，特定任务的体系结构其实并不是必须的，transformer的self-attention模块就已经能够满足目前的需要。

<未完待续>









