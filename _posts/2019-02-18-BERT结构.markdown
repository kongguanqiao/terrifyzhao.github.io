---
layout: post
title: 'BERT原理说明'
subtitle: 'BERT原理'
date: 2019-02-18
categories: NLP
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2019-01-11-BERT%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/cover.jpg'
tags: NLP
---



## **简介**

之前的文章从attention讲解到了transformer，本文将会针对目前大热的BERT进行讲解，bert的内部结构主要是transformer，如果您对transformer并不了解，请参阅我之前的博文。   

从创新的角度来看，bert其实并没有过多的结构方面的创新点，其和GPT一样均是采用的transformer的结构，但相对于GPT来说，其实双向结构的，而GPT是单向的


