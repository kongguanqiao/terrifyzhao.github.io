---
layout: post
title: 'PCA主成分分析'
subtitle: 'PCA推导全过程'
date: 2018-02-28
categories: 机器学习
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-28-%E5%86%B3%E7%AD%96%E6%A0%91/cover.jpeg'
tags: 机器学习
---

## 前言

之前看过的大部分PCA博文都是只介绍了PCA的计算流程，对其中的推导过程与原理并没有详细介绍，这篇文章的目的是介绍PCA的基本数学原理，手推PCA每一个步骤，帮助读者了解PCA的工作机制是什么。对于文中的某些数学公式，可能会对某些读者产生不适，我会尽可能的用白话把其中原理讲解的通俗易懂。

## PCA简介

PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。通俗讲就是将高纬度数据变为低维度，例如基于电商的用户数据可能有上亿维，我们可以采用PCA把维度从亿级别降低到万级别。

## 向量的内积

在开始下面的内容之前，我们需要弄懂几个基本概念，首相是向量的内积。

向量的内积我们在高中就已经学过,两个维数相同的向量的内积被定义为：

$$(a_1,a_2,...,a_n)·(b_1,b_2,...b_n)^T = a_1b_1+a_2b_2+...+a_nb_n$$

这个定义很好理解，那么内积的几何意义是什么呢，我们看个图

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca1.jpg' width=300>

内积的另一种我们熟悉的表述方法为向量的模乘上向量之间的夹角的余弦值，即：

$$A·B=|A||B|cos(a)$$

如果我们假设B的模为1，即单位向量，那么：

$$A·B=|A|cos(a)$$

这里我们可以发现，内积其实就是A向量在B向量的方向上的投影的长度。

## 散度

接下来我们考虑一个问题：对于空间中的所有样本点，如何找到一个超平面对所有的样本进行恰当的表达？对于二维空间，也就是说我们应该把样本点投影到x轴还是y轴进行降维。

对于这个问题，我们需要找到的超平面需满足**最大可分性：样本点在这个超平面上的投影能尽可能分开**，这个分开的程度我们称之为散度（散度可以采用方差或协方差来衡量，在机器学中，样本的方差较大时，对最终的结果影响会忧与方差较小的样本）如图，对于方差0.2的超平面，即为我们需要的。


<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca2.png' width=300>


这里我们再简单补充下协方差的知识：

方差是用来形容单个维度的样本的波动程度，协方差是指多个维度的样本数据的相关性，其计算公式为：

$$Cov(X,Y) = \frac{\sum_{i=1}^n(X_i- \overline X)(Y_i- \overline Y)}{(n-1)}$$


其中$Cov(X,Y)\in (-1,1)$，绝对值越大说明相关性越高。注意，协方差不等于相关系数，相关系数是协方差除标准差，相关系数的相除操作把样本的单位去除了，因此结果更加标准化一些，实际含义类似。

## 协方差矩阵

既然我们已经知道了怎么去找最合适的超平面，那下一步就是对所有的超平面都做一次散度的计算。因此，我们需要构建协方差矩阵


