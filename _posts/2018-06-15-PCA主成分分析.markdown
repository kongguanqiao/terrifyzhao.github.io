---
layout: post
title: 'PCA主成分分析'
subtitle: 'PCA推导全过程'
date: 2018-06-15
categories: 机器学习
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/cover.jpeg'
tags: 机器学习
---

## 前言

之前看过的大部分PCA博文都是只介绍了PCA的计算流程，对其中的推导过程与原理并没有详细介绍，这篇文章的目的是从数学的角度，手推PCA每一个步骤，帮助读者了解PCA的工作机制是什么。文中的某些数学公式，可能会对某些读者产生不适，我会尽可能的用白话把其中原理讲解的通俗易懂。

## PCA简介

PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。通俗讲就是将高维度数据变为低维度，例如基于电商的用户数据可能有上亿维，我们可以采用PCA把维度从亿级别降低到万级别或千级别。

## 向量的内积

在开始下面的内容之前，我们需要弄懂几个基本概念，首相是向量的内积。

向量的内积我们在高中就已经学过,两个维数相同的向量的内积被定义为：

$$(a_1,a_2,...,a_n)·(b_1,b_2,...b_n)^T = a_1b_1+a_2b_2+...+a_nb_n$$

这个定义很好理解，那么内积的几何意义是什么呢，我们看个图

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca1.jpg' width=300>

内积的另一种我们熟悉的表述方法为向量的模乘上向量之间的夹角的余弦值，即：

$$A·B=|A||B|cos(a)$$

如果我们假设B的模为1，即单位向量，那么：

$$A·B=|A|cos(a)$$

这里我们可以发现，内积其实就是A向量在B向量的方向上的投影的长度。

## 散度

接下来我们考虑一个问题：对于空间中的所有样本点，如何找到一个超平面对所有的样本进行恰当的表达？对于二维空间，也就是说我们应该把样本点投影到x轴还是y轴进行降维。

对于这个问题，我们需要找到的超平面需满足**最大可分性：样本点在这个超平面上的投影能尽可能分开**，这个分开的程度我们称之为散度（散度可以采用方差或协方差来衡量，在机器学中，样本的方差较大时，对最终的结果影响会优于方差较小的样本）如图，对于方差0.2的超平面散度高于方差为0.045的超平面，因此0.2方差的超平面即为我们需要的。


<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca2.png' width=300>


这里我们再简单补充下协方差的知识：

方差是用来形容单个维度的样本的波动程度，协方差是指多个维度的样本数据的相关性，其计算公式为：

$$Cov(X,Y) = \frac{\sum_{i=1}^n(X_i- \overline X)(Y_i- \overline Y)}{(n-1)}$$


其中$Cov(X,Y)\in (-1,1)$，绝对值越大说明相关性越高。注意，协方差不等于相关系数，相关系数是协方差除标准差，相关系数的相除操作把样本的单位去除了，因此结果更加标准化一些，实际含义类似。

## 协方差矩阵

**PCA的首要目标是让投影后的散度最大**，因此我们要对所有的超平面的投影都做一次散度的计算，并找到最大散度的超平面。为了方便计算我们需要构建协方差矩阵。

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca3.jpg' width=300>

图中是一个三维的协方差矩阵，其中对角线是样本本身的协方差即方差，非对角线是不同样本之间的协方差。

注意，在PCA中，我们会对所有的数据进行中心化的操作，中心化后数据的均值为0，即：

$$x_i = x_i - \frac{1}{m} \sum_{i=1}^mx_i$$

根据我们上文提到的协方差计算公式，我们可以得到数据样本的协方差矩阵为：

$$
\begin{align}
Cov(X_i) &= \frac{1}{m} \sum_{i=1}^m (X_i-\overline X_i)^2\\
&= \frac{1}{m} \sum_{i=1}^m (X_i)^2\\
&= \frac{1}{m} \sum_{i=1}^m X_i·X_i^T\\
\end{align}
$$

我们设可投影的超平面为$V$，我们要求投影的协方差，是不是可以根据我们第一条提到的向量的内积呢？因此我们可以得到投影后的值为$V^T ·X_i$，我们把投影后的方差计算一下

$$S^2 = \frac{1}{m} \sum_{i=1}^m (V^T X_i-E(V^T X))^2$$

这里我们进一步的做中心化操作，因此期望值为0，所以有：

$$
\begin{align}
S^2 &= \frac{1}{m} \sum_{i=1}^m (V^T X_i)^2\\
&= \frac{1}{m} \sum_{i=1}^m V^T X_i X_i^T V\\
&= \frac{1}{m} \sum_{i=1}^m X_iX_i^T V V^T\\
\end{align}
$$

仔细看，投影的方差即是原数据样本的协方差矩阵乘$V V^T$。为了后续表述方便，我们设元数据样本的协方差矩阵为C，即：

$$S^2 = V^TCV$$
    

## 最大化散度

到了这一步，我们获得了投影的散度的计算方法。我们再看下PCA的首要目标：**让投影后的散度最大**，接下来就是我们熟悉的优化问题了，不过这里有一个限制条件，即超平面向量的模为1，目标即：

$$
\begin{equation}
argmax\ V^TCV\\
s.t.|V|=1\\
\end{equation}
$$

对于有限制条件，我们采用拉格朗日乘子法来解决，即：

$$L = V^TCV -\alpha(VV^T-1)$$

对于求极值的问题，当然是求导啦，这里我们对V求导，即：

$$\frac{\alpha L}{\alpha v} = 2CV-2\alpha V$$

我们令导数为0，即：

$$CV = \alpha V$$

我们把最后的值带入到L中，即：

$$L = \alpha$$

最后散度的值只由$\alpha$来决定，因此$\alpha$的值越大，散度越大，而对于$CV = \alpha V$这个公式是不是很熟悉，没错就是特征值的定义式，其中$\alpha$即是特征值，V即是特征向量。

## 特征值定义式

接下来我们讲讲$CV = \alpha V$这个公式。相信大家对于这个式子非常熟悉，但是你真正的理解这个式子了吗？特征向量和特征值到底有什么意义呢？

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca5.jpg' width=600>

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca4.jpg' width=400>

首先，我们要明确一个矩阵和一个向量相乘有什么意义？从图中我们可以看出一个矩阵和一个向量相乘的意义在于对该向量做个旋转或伸缩变换（$X_3$为特征向量，其它为非特征向量）。从图中我们又发现：

+ 一个矩阵和该矩阵的非特征向量相乘是对该向量的旋转变换。

+ 一个矩阵和该矩阵的特征向量相乘是对该向量的伸缩变换。

+ 一个实数和一个向量相乘是对一个向量的伸缩变换。


通过以上分析，我们就明白了特征值和特征向量是什么了？特征向量就是一个在矩阵A的变换下没有旋转只是伸缩变换，那到底伸缩了多少倍呢？伸缩了“特征值”倍。


接下来我们看一个很重要的特性，如果一个对称矩阵的特征值不同，则其相应的所有的特征向量正交，即相互垂直。

在我们求出最大的特征值之后，我们可以根据这个定义，找到其他特征向量并根据散度进行排序，我们把对应的特征向量组成一个矩阵,并根绝我们想最终降的维度$d$来得到最终的投影矩阵$W^* = (w_1,w_2,...,w_d)$,把原始样本数据与投影矩阵做矩阵乘法，即可得到降维后的结果。



