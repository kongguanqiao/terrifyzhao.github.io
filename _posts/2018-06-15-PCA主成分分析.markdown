---
layout: post
title: 'PCA主成分分析'
subtitle: 'PCA推导全过程'
date: 2018-02-28
categories: 机器学习
cover: 'https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/cover.jpeg'
tags: 机器学习
---

## 前言

之前看过的大部分PCA博文都是只介绍了PCA的计算流程，对其中的推导过程与原理并没有详细介绍，这篇文章的目的是介绍PCA的基本数学原理，手推PCA每一个步骤，帮助读者了解PCA的工作机制是什么。对于文中的某些数学公式，可能会对某些读者产生不适，我会尽可能的用白话把其中原理讲解的通俗易懂。

## PCA简介

PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。通俗讲就是将高纬度数据变为低维度，例如基于电商的用户数据可能有上亿维，我们可以采用PCA把维度从亿级别降低到万级别。

## 向量的内积

在开始下面的内容之前，我们需要弄懂几个基本概念，首相是向量的内积。

向量的内积我们在高中就已经学过,两个维数相同的向量的内积被定义为：

$$(a_1,a_2,...,a_n)·(b_1,b_2,...b_n)^T = a_1b_1+a_2b_2+...+a_nb_n$$

这个定义很好理解，那么内积的几何意义是什么呢，我们看个图

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca1.jpg' width=300>

内积的另一种我们熟悉的表述方法为向量的模乘上向量之间的夹角的余弦值，即：

$$A·B=|A||B|cos(a)$$

如果我们假设B的模为1，即单位向量，那么：

$$A·B=|A|cos(a)$$

这里我们可以发现，内积其实就是A向量在B向量的方向上的投影的长度。

## 散度

接下来我们考虑一个问题：对于空间中的所有样本点，如何找到一个超平面对所有的样本进行恰当的表达？对于二维空间，也就是说我们应该把样本点投影到x轴还是y轴进行降维。

对于这个问题，我们需要找到的超平面需满足**最大可分性：样本点在这个超平面上的投影能尽可能分开**，这个分开的程度我们称之为散度（散度可以采用方差或协方差来衡量，在机器学中，样本的方差较大时，对最终的结果影响会忧与方差较小的样本）如图，对于方差0.2的超平面，即为我们需要的。


<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca2.png' width=300>


这里我们再简单补充下协方差的知识：

方差是用来形容单个维度的样本的波动程度，协方差是指多个维度的样本数据的相关性，其计算公式为：

$$Cov(X,Y) = \frac{\sum_{i=1}^n(X_i- \overline X)(Y_i- \overline Y)}{(n-1)}$$


其中$Cov(X,Y)\in (-1,1)$，绝对值越大说明相关性越高。注意，协方差不等于相关系数，相关系数是协方差除标准差，相关系数的相除操作把样本的单位去除了，因此结果更加标准化一些，实际含义类似。

## 协方差矩阵

接下来我们就按照上文提到的方案去寻找超平面。

首先对所有的超平面都做一次散度的计算。因此，我们需要构建协方差矩阵。

<img src='https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-06-15-PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/pca3.jpg' width=300>

图中是一个三维的协方差矩阵，其中对角线是样本本身的协方差即方差，非对角线是不同样本之间的协方差。

注意，在PCA中，我们会对所有的数据进行中心化的操作，中心化后数据的均值为0，即：

$$x_i = x_i - \frac{1}{m} \sum_{i=1}^mx_i$$

根据我们上文提到的协方差计算公式，我们可以得到协方差矩阵为：

$$
\begin{align}
Cov(X_i) &= \frac{1}{m} \sum_{i=1}^m (X_i-\overline X_i)^2\\
&= \frac{1}{m} \sum_{i=1}^m (X_i)^2\\
&= \frac{1}{m} \sum_{i=1}^m X_i·X_i^T\\
\end{align}
$$

接下来我们再计算下投影之后的样本点，我们设可投影的维度为$V$，根据我们第一条提到的向量的内积，我们可以得到投影后的值为$V^T ·X_i$，我们把投影后的方差计算一下

$$S^2 = \frac{1}{m} \sum_{i=1}^m (V^T X_i-E(V^T X))^2$$

这里我们进一步的做中心化操作，因此期望值为0，所以有：

$$
\begin{align}
S^2 &= \frac{1}{m} \sum_{i=1}^m (V^T X_i)^2\\
&= \frac{1}{m} \sum_{i=1}^m V^T X_i X_i^T V\\
&= \frac{1}{m} \sum_{i=1}^m X_iX_i^T V V^T\\
\end{align}
$$

仔细看，简化后的方差即是协方差矩阵乘$V V^T$。为了后续表述方便，我们设协方差矩阵为C，即：

$$S^2 = V^TCV$$
    

## 最大化散度

方差我们拿到了，根据上文的推导，我们要让散度最大，即方差最大，并且投影的坐标轴范数为1即摸为1，那我们下一步的目标即是：

$$
\begin{equation}
argmax\ V^TCV\\
s.t.|V|=1\\
\end{equation}
$$

对于有限制条件的优化问题，我们采用拉格朗日乘子法来解决，即：

$$L = V^TCV -\alpha(VV^T-1)$$

对于求极值的问题，当然是求导啦，这里我们对V求导，即：

$$\frac{\alpha L}{\alpha v} = 2CV-2\alpha V$$

我们令导数为0，即：

$$CV = \alpha V$$

我们把最后的值带入到L中，即：

$$L = \alpha$$

最后散度的值只由$\alpha$来决定，因此$\alpha$的值越大，散度越大，而对于$CV = \alpha V$这个公式其实就是特征值的定义式，其中$\alpha$即是特征值，V即是特征向量。

## 特征值分解

有了特征值的定义式，我们即可求出最大的特征值，根据特征值分解的定义，其他特征值是垂直于我们找到的最大的特征值的，根据这个定义，我们可以求出所有的特征值，并对特征值进行从大到小的排序，我们把对应的特征向量组成一个矩阵,并根绝我们想最终降的维度$d$来得到最终的投影矩阵$W^* = (w_1,w_2,...,w_d)$,
把原始样本数据与投影矩阵做矩阵乘法，即可得到降维后的结果。



