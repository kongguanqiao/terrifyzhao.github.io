---
layout: post
title: 'LSTM网络'
subtitle: 'LSTM网络讲解'
date: 2018-02-13
categories: 神经网络
cover: ''
tags: 神经网络 RNN
---


## LSTM 网络
长短期记忆网络（Long Short Term）通常也被简称为LSTM，是一种特殊类型的 RNN，能够学习长期的依赖关系。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！

所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm1.png" width="630" height="240"/>

LSTM 同样是这样的结构,不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm2.png" width="630" height="240"/>

其次，LSTM添加了一个状态信息，也就是下图这条向右的线

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm3.png" width="600" height="180"/>

状态会沿着整条链条传送，而只有少数地方有一些线性交互。信息如果以这样的方式传递，实际上会保持不变。

LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到状态的能力。理解“门”就是LSTM网络的关键，而LSTM的“门”分为了三种，遗忘门，输入门，输出门，接下来我们就详细介绍下这三个门。

### 遗忘门
首先，LSTM 的第一步需要决定我们需要抛弃哪些信息。这个决定是依靠遗忘门来实现的，具体的结构如图，输入是$h_{(t-1)}$与$x{(t)}$,经过sigmoid函数后，输出了一个0-1之间的值，0代表完全遗忘，1代表完全保留。

我们拿预测单词的语言模型来举例，当前状态可能会需要考虑主语的性别，这样才能找到正确的代词。因此如果我们设定，如果看到了一个新的主语，就“忘记“旧的主语所代表的性别。

例如：他今天有事，所以我...当处理到“我“的时候选择性的忘记前面的“他“，或者说减小这个词对后面词的作用。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm4.png" width="600" height="180"/>

### 输入门
经过遗忘门之后，我们需要决定什么样的信息应该被存储起来。这个过程主要分两步。首先是$sigmoid$，同样输出一个0-1的值，从而决定我们需要更新哪些值。随后，$tanh$生成了一个新的候选向量$\tilde{C}$，将这两个值相乘最后得到需要输入的值。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm5.png" width="600" height="180"/>

接下来，我们就可以更新状态了。将旧状态与$ft$相乘，忘记此前我们想要忘记的内容，然后加上输出值$\tilde{C}$。得到的结果便是新的状态值。

<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm6.png" width="600" height="180"/>

在我们语言模型的例子中，我们希望增加新的主语的类别到细胞状态中，来替代旧的需要忘记的主语。 

例如：他今天有事，所以我...当处理到“我”这个词的时候，就会把主语“我”更新到细胞中去。


### 输出门
最后，我们需要输出状态，即输出门。首先，我们会运行一个 sigmoid 层决定 cell 状态输出哪一部分。随后，我们把 cell 状态通过 tanh 函数，将输出值保持在-1 到 1 间。之后，我们再乘以 sigmoid 门的输出值，就可以得到结果了。

对于语言模型的例子，当它只看到一个主语时，就可能会输出与动词相关的信息。比如它会输出主语是单数还是复数。这样的话，如果后面真的出现了动词，我们就可以确定它的形式了。

因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。 

例如：上面的例子，当处理到‘’我‘’这个词的时候，可以预测下一个词，是动词的可能性较大，而且是第一人称。 会把前面的信息保存到隐层中去。



<img src="https://raw.githubusercontent.com/terrifyzhao/terrifyzhao.github.io/master/assets/img/2018-02-13-LSTM%E7%BD%91%E7%BB%9C/lstm7.png" width="600" height="180"/>

##  LSTM变体
我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。这里，我们简单介绍下GRU.

GRU是由 Cho, et al. (2014) 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。





